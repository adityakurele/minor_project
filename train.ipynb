{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from discriminator import Discriminator\n",
    "from generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "seed = 3079             # Random Seed Noise\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Common Hyperparameters\n",
    "params = {\n",
    "    \"bsize\" : 64,       # Training batch size\n",
    "    'imsize' : 64,      # Imagesize, preprocess to this\n",
    "    'nc' : 3,           # RGB\n",
    "    'nz' : 100,         # Input latent vector\n",
    "    'ngf' : 64,         # Number of generator filters/featurs\n",
    "    'ndf' : 64,         # Number of discriminator filters/features\n",
    "    'nepochs' : 50,     # Number of training epochs.\n",
    "    'lr' : 0.0002,      # Learning rate\n",
    "    'beta1' : 0.5,      # First Decay rate Adam\n",
    "    'save_epoch' : 2}   # Save every 2 epoch\n",
    "\n",
    "# Figure out how to use Intel Iris Xe Pytorch... why can't colab just support python 10...\n",
    "device = torch.device(\"cuda:0\" if(torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './dataset/'\n",
    "\n",
    "def load_dataset(params):\n",
    "    \"\"\"\n",
    "    PyTorch DataLoader object\n",
    "    \"\"\"\n",
    "    # Preprocessing just in case(transparent images)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(params['imsize']),\n",
    "        transforms.CenterCrop(params['imsize']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "            (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Create the dataset.\n",
    "    dataset = dset.ImageFolder(root=root, transform=transform)\n",
    "\n",
    "    # Create the dataloader.\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "        batch_size=params['bsize'],\n",
    "        shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Get the data.\n",
    "dataloader = load_dataset(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(w):\n",
    "    \"\"\"\n",
    "    Weights to mean=0.0, stddev=0.2, classname w(weights)\n",
    "    \"\"\"\n",
    "    classname = w.__class__.__name__\n",
    "    if classname.find('conv') != -1:\n",
    "        nn.init.normal_(w.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('bn') != -1:\n",
    "        nn.init.normal_(w.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(w.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the generator.\n",
    "gen_obj = Generator(params).to(device)\n",
    "gen_obj.apply(weights_init)\n",
    "print(gen_obj)\n",
    "\n",
    "# Create the discriminator.\n",
    "dis_obj = Discriminator(params).to(device)\n",
    "dis_obj.apply(weights_init)\n",
    "print(dis_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizerD = optim.Adam(dis_obj.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "optimizerG = optim.Adam(gen_obj.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []   # Stores generated images as training progresses.\n",
    "gen_loss = []   # Stores generator losses during training.\n",
    "dis_loss = []   # Stores discriminator losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      ")\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      ")\n",
      "Starting Training Loop...\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size, ), real_label, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     53\u001b[0m output \u001b[39m=\u001b[39m netD(real_data)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m errD_real \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Calculate gradients for backpropagation.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m errD_real\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\modules\\loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\functional.py:3083\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3080\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3081\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3083\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "# Create the generator.\n",
    "netG = Generator(params).to(device)\n",
    "# Apply the weights_init() function to randomly initialize all\n",
    "# weights to mean=0.0, stddev=0.2\n",
    "netG.apply(weights_init)\n",
    "# Print the model.\n",
    "print(netG)\n",
    "\n",
    "# Create the discriminator.\n",
    "netD = Discriminator(params).to(device)\n",
    "# Apply the weights_init() function to randomly initialize all\n",
    "# weights to mean=0.0, stddev=0.2\n",
    "netD.apply(weights_init)\n",
    "# Print the model.\n",
    "print(netD)\n",
    "\n",
    "# Binary Cross Entropy loss function.\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(64, params['nz'], 1, 1, device=device)\n",
    "\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Optimizer for the discriminator.\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "# Optimizer for the generator.\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=params['lr'], betas=(params['beta1'], 0.999))\n",
    "\n",
    "# Stores generated images as training progresses.\n",
    "img_list = []\n",
    "# Stores generator losses during training.\n",
    "G_losses = []\n",
    "# Stores discriminator losses during training.\n",
    "D_losses = []\n",
    "\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "print(\"-\"*25)\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Transfer data tensor to GPU/CPU (device)\n",
    "        real_data = data[0].to(device)\n",
    "        # Get batch size. Can be different from params['nbsize'] for last batch in epoch.\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        # Make accumalated gradients of the discriminator zero.\n",
    "        netD.zero_grad()\n",
    "        # Create labels for the real data. (label=1)\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for backpropagation.\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        # Sample random data from a unit normal distribution.\n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "        # Generate fake data (images).\n",
    "        fake_data = netG(noise)\n",
    "        # Create labels for fake data. (label=0)\n",
    "        label.fill_(fake_label  )\n",
    "        # Calculate the output of the discriminator of the fake data.\n",
    "        # As no gradients w.r.t. the generator parameters are to be\n",
    "        # calculated, detach() is used. Hence, only gradients w.r.t. the\n",
    "        # discriminator parameters will be calculated.\n",
    "        # This is done because the loss functions for the discriminator\n",
    "        # and the generator are slightly different.\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate gradients for backpropagation.\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # Net discriminator loss.\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update discriminator parameters.\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Make accumalted gradients of the generator zero.\n",
    "        netG.zero_grad()\n",
    "        # We want the fake data to be classified as real. Hence\n",
    "        # real_label are used. (label=1)\n",
    "        label.fill_(real_label)\n",
    "        # No detach() is used here as we want to calculate the gradients w.r.t.\n",
    "        # the generator this time.\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        # Gradients for backpropagation are calculated.\n",
    "        # Gradients w.r.t. both the generator and the discriminator\n",
    "        # parameters are calculated, however, the generator's optimizer\n",
    "        # will only update the parameters of the generator. The discriminator\n",
    "        # gradients will be set to zero in the next iteration by netD.zero_grad()\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update generator parameters.\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Check progress of training.\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on a fixed noise.\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Save the model.\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model/model_epoch_{}.pth'.format(epoch))\n",
    "\n",
    "# Save the final trained model.\n",
    "torch.save({\n",
    "            'generator' : netG.state_dict(),\n",
    "            'discriminator' : netD.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model/model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((b_size, ), real_label, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m output \u001b[39m=\u001b[39m dis_obj(real_data)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m dis_realerror \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[0;32m     15\u001b[0m \u001b[39m# backprop gradient\u001b[39;00m\n\u001b[0;32m     16\u001b[0m dis_realerror\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\modules\\loss.py:613\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 613\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\hardi\\anaconda3\\envs\\aipet\\lib\\site-packages\\torch\\nn\\functional.py:3083\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3080\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3081\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3083\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "iters = 0\n",
    "\n",
    "for epoch in range(params['nepochs']):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        \n",
    "        # Make accumalated gradients of the discriminator zero.\n",
    "        dis_obj.zero_grad()\n",
    "        # Real data = 1\n",
    "        label = torch.full((b_size, ), real_label, device=device)\n",
    "        output = dis_obj(real_data).view(-1)\n",
    "        dis_realerror = criterion(output, label)\n",
    "\n",
    "        # backprop gradient\n",
    "        dis_realerror.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        # Sample random data from a unit normal distribution.\n",
    "        noise = torch.randn(b_size, params['nz'], 1, 1, device=device)\n",
    "\n",
    "        # Generate fake data (images).\n",
    "        fake_data = gen_obj(noise)\n",
    "\n",
    "        # Fake data = 0\n",
    "        label.fill_(fake_label  )\n",
    "        output = dis_obj(fake_data.detach()).view(-1)\n",
    "        dis_fakeerror = criterion(output, label)\n",
    "\n",
    "        # Calculate gradients for backpropagation.\n",
    "        dis_fakeerror.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # Net discriminator loss.\n",
    "        loss_d = dis_realerror + dis_fakeerror\n",
    "\n",
    "        # Update discriminator parameters.\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Make accumalted gradients of the generator zero.\n",
    "        gen_obj.zero_grad()\n",
    "        # We want the fake data to become real so pass 1\n",
    "        label.fill_(real_label)\n",
    "\n",
    "        output = dis_obj(fake_data).view(-1)\n",
    "        loss_g = criterion(output, label)\n",
    "        loss_g.backward()\n",
    "\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update generator parameters.\n",
    "        optimizerG.step()\n",
    "\n",
    "        # training progress\n",
    "        if i%50 == 0:\n",
    "            print(torch.cuda.is_available())\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, params['nepochs'], i, len(dataloader),\n",
    "                     loss_d.item(), loss_g.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save the losses for plotting.\n",
    "        gen_loss.append(loss_g.item())\n",
    "        dis_loss.append(loss_d.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on a fixed noise.\n",
    "        if (iters % 100 == 0) or ((epoch == params['nepochs']-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake_data = gen_obj(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake_data, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    # Save the model.\n",
    "    if epoch % params['save_epoch'] == 0:\n",
    "        torch.save({\n",
    "            'generator' : gen_obj.state_dict(),\n",
    "            'discriminator' : dis_obj.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model/model_epoch_{}.pth'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model.\n",
    "torch.save({\n",
    "            'generator' : gen_obj.state_dict(),\n",
    "            'discriminator' : dis_obj.state_dict(),\n",
    "            'optimizerG' : optimizerG.state_dict(),\n",
    "            'optimizerD' : optimizerD.state_dict(),\n",
    "            'params' : params\n",
    "            }, 'model/model_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
